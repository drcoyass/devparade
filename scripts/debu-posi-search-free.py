#!/usr/bin/env python3
"""
PosiDev Free Search - ç„¡æ–™ã§ãƒã‚¬ãƒ‡ãƒ–ãƒ„ã‚¤ãƒ¼ãƒˆã‚’æ¤œç´¢ & ãƒã‚¸ãƒ‡ãƒ–è‡ªå‹•è¿”ä¿¡

X API Freeãƒ—ãƒ©ãƒ³ã§ã¯æ¤œç´¢APIãŒä½¿ãˆãªã„ï¼ˆ$100/æœˆã®BasicãŒå¿…è¦ï¼‰ã€‚
ãã“ã§ä»¥ä¸‹ã®ç„¡æ–™æ‰‹æ³•ã§ãƒã‚¬ãƒ‡ãƒ–ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç™ºè¦‹ã—ã€X APIã§è‡ªå‹•è¿”ä¿¡ã™ã‚‹:

æ–¹å¼1: Googleæ¤œç´¢ (site:x.com "ãƒ‡ãƒ–" "è¾›ã„") â†’ ãƒ„ã‚¤ãƒ¼ãƒˆURLå–å¾— â†’ è¿”ä¿¡
æ–¹å¼2: ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ç›£è¦– (@dev_paradeã¸ã®è¿”ä¿¡ã‚’è‡ªå‹•è¿”ä¿¡)
æ–¹å¼3: ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ç›£è¦– (#ãƒã‚¸ãƒ‡ãƒ– / #ãƒ‡ãƒ– / #å¤ªã£ãŸ)

â€» X APIã®æŠ•ç¨¿æ©Ÿèƒ½ã¯Freeãƒ—ãƒ©ãƒ³ã§ã‚‚ä½¿ãˆã‚‹
"""

import os
import sys
import re
import json
import random
import time
import urllib.parse
from datetime import datetime, timezone, timedelta

try:
    import tweepy
except ImportError:
    print("tweepy not installed")
    sys.exit(1)

try:
    import requests
    from bs4 import BeautifulSoup
    HAS_SCRAPER = True
except ImportError:
    HAS_SCRAPER = False

API_KEY = os.environ.get("X_API_KEY")
API_SECRET = os.environ.get("X_API_SECRET")
ACCESS_TOKEN = os.environ.get("X_ACCESS_TOKEN")
ACCESS_SECRET = os.environ.get("X_ACCESS_SECRET")
BEARER_TOKEN = os.environ.get("X_BEARER_TOKEN")

FOUND_IDS_FILE = "replied_tweet_ids.json"
LAST_MENTION_ID_FILE = "last_monitor_id.txt"

MEMBERS = [
    {"name": "ãƒãƒ³ã‚µãƒ åˆ¤æ²»", "role": "Vo./Leader", "weight": "90kgè¶…"},
    {"name": "COYASS", "role": "MC", "weight": "90kgè¶…"},
    {"name": "ugazin", "role": "Gt./ä½œæ›²", "weight": "90kgè¶…"},
    {"name": "ãºãƒ¼", "role": "Ba.", "weight": "90kgè¶…"},
    {"name": "TAH", "role": "Dr.", "weight": "90kgè¶…"},
]

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰DBã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from posideb_keywords import ALL_GOOGLE_QUERIES as GOOGLE_QUERIES_ALL
from posideb_keywords import select_response


def get_write_client():
    if not all([API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_SECRET]):
        return None
    return tweepy.Client(
        consumer_key=API_KEY,
        consumer_secret=API_SECRET,
        access_token=ACCESS_TOKEN,
        access_token_secret=ACCESS_SECRET,
        wait_on_rate_limit=True,
    )


def get_read_client():
    if BEARER_TOKEN:
        return tweepy.Client(bearer_token=BEARER_TOKEN, wait_on_rate_limit=True)
    return get_write_client()


def load_replied_ids():
    try:
        with open(FOUND_IDS_FILE, "r") as f:
            return set(json.load(f))
    except (FileNotFoundError, json.JSONDecodeError):
        return set()


def save_replied_ids(ids):
    # æœ€æ–°500ä»¶ã®ã¿ä¿æŒ
    ids_list = list(ids)[-500:]
    with open(FOUND_IDS_FILE, "w") as f:
        json.dump(ids_list, f)


def get_last_mention_id():
    try:
        with open(LAST_MENTION_ID_FILE, "r") as f:
            return f.read().strip()
    except FileNotFoundError:
        return None


def save_last_mention_id(tweet_id):
    with open(LAST_MENTION_ID_FILE, "w") as f:
        f.write(str(tweet_id))


# ===== æ–¹å¼1: Googleæ¤œç´¢ã§ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç™ºè¦‹ =====
def search_google_for_tweets():
    """Googleæ¤œç´¢ã§ãƒã‚¬ãƒ‡ãƒ–ãƒ„ã‚¤ãƒ¼ãƒˆã®URLã‚’å–å¾—"""
    if not HAS_SCRAPER:
        print("   âš ï¸ requests/beautifulsoup4 æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
        return []

    found_tweet_ids = []
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "ja,en;q=0.9",
    }

    # ãƒ©ãƒ³ãƒ€ãƒ ã«2ã‚¯ã‚¨ãƒªã ã‘å®Ÿè¡Œï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿ï¼‰
    queries = random.sample(GOOGLE_QUERIES_ALL, min(2, len(GOOGLE_QUERIES_ALL)))

    for query in queries:
        try:
            # Googleæ¤œç´¢ï¼ˆç›´è¿‘24æ™‚é–“: tbs=qdr:dï¼‰
            url = f"https://www.google.com/search?q={urllib.parse.quote(query)}&tbs=qdr:d&num=10"
            print(f"   ğŸ” Google: {query[:50]}...")

            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                print(f"   âš ï¸ Googleå¿œç­”: {resp.status_code}")
                continue

            soup = BeautifulSoup(resp.text, "html.parser")

            # Googleæ¤œç´¢çµæœã‹ã‚‰x.com/twitter.comã®URLã‚’æŠ½å‡º
            for link in soup.find_all("a", href=True):
                href = link["href"]
                # Googleçµæœã®URLãƒ‘ã‚¿ãƒ¼ãƒ³
                match = re.search(r'(?:x\.com|twitter\.com)/(\w+)/status/(\d+)', href)
                if match:
                    username = match.group(1)
                    tweet_id = match.group(2)
                    if username not in ("dev_parade", "i", "search", "hashtag"):
                        found_tweet_ids.append({
                            "id": tweet_id,
                            "username": username,
                            "source": "google",
                        })
                        print(f"   ğŸ“Œ ç™ºè¦‹: @{username}/status/{tweet_id}")

            time.sleep(2)  # Google ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿

        except Exception as e:
            print(f"   âŒ Googleæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return found_tweet_ids


# ===== æ–¹å¼2: ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ç›£è¦– =====
def check_mentions(read_client, write_client):
    """@dev_paradeã¸ã®ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã‚’ç›£è¦–"""
    results = []

    try:
        me = write_client.get_me()
        if not me.data:
            return []
        my_id = me.data.id
        my_username = me.data.username

        last_id = get_last_mention_id()
        kwargs = {
            "id": my_id,
            "max_results": 20,
            "tweet_fields": ["created_at", "author_id", "text"],
            "user_fields": ["username"],
            "expansions": ["author_id"],
        }
        if last_id:
            kwargs["since_id"] = last_id

        result = read_client.get_users_mentions(**kwargs)
        if not result.data:
            print("   ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ãªã—")
            return []

        users = {}
        if result.includes and "users" in result.includes:
            for user in result.includes["users"]:
                users[user.id] = user.username

        newest_id = last_id
        for tweet in result.data:
            username = users.get(tweet.author_id, "unknown")
            if username == my_username:
                continue

            results.append({
                "id": str(tweet.id),
                "username": username,
                "text": tweet.text,
                "source": "mention",
            })
            print(f"   ğŸ“© ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³: @{username}")

            if newest_id is None or int(tweet.id) > int(newest_id or 0):
                newest_id = str(tweet.id)

        if newest_id and newest_id != last_id:
            save_last_mention_id(newest_id)

    except Exception as e:
        print(f"   âš ï¸ ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³å–å¾—: {e}")

    return results


# ===== ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°å–å¾— & è¿”ä¿¡ =====
def reply_to_tweets(write_client, read_client, tweet_targets, replied_ids):
    """ç™ºè¦‹ã—ãŸãƒ„ã‚¤ãƒ¼ãƒˆã«ãƒã‚¸ãƒ‡ãƒ–è¿”ä¿¡"""
    results = []
    reply_count = 0

    for target in tweet_targets:
        if reply_count >= 5:  # 1å›ã®å®Ÿè¡Œã§æœ€å¤§5ä»¶
            break

        tweet_id = target["id"]
        if tweet_id in replied_ids:
            print(f"   â­ï¸ è¿”ä¿¡æ¸ˆã¿ã‚¹ã‚­ãƒƒãƒ—: {tweet_id}")
            continue

        username = target["username"]
        tweet_text = target.get("text", "")

        # GoogleçµŒç”±ã®å ´åˆã€ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡ãŒãªã„ã®ã§APIã§å–å¾—ã‚’è©¦ã¿ã‚‹
        if not tweet_text and read_client:
            try:
                tweet_data = read_client.get_tweet(tweet_id, tweet_fields=["text"])
                if tweet_data.data:
                    tweet_text = tweet_data.data.text
            except Exception:
                tweet_text = ""

        response = select_response(tweet_text or "ãƒ‡ãƒ–")
        member = random.choice(MEMBERS)
        reply_text = f"@{username} {response}"

        result = {
            "id": tweet_id,
            "username": username,
            "text": tweet_text[:100] if tweet_text else "(Googleæ¤œç´¢ã§ç™ºè¦‹)",
            "response": response,
            "member": member,
            "status": "pending",
            "source": target.get("source", "unknown"),
        }

        try:
            write_client.create_tweet(
                text=reply_text,
                in_reply_to_tweet_id=tweet_id,
            )
            result["status"] = "sent"
            replied_ids.add(tweet_id)
            reply_count += 1
            print(f"   âœ… ãƒã‚¸ãƒ‡ãƒ–è¿”ä¿¡ â†’ @{username}: {response[:40]}...")
            time.sleep(3)
        except tweepy.errors.Forbidden as e:
            error_msg = str(e)
            if "duplicate" in error_msg.lower():
                print(f"   â­ï¸ é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: @{username}")
                replied_ids.add(tweet_id)
            else:
                print(f"   âŒ è¿”ä¿¡å¤±æ•—(403): {e}")
            result["status"] = f"failed: {e}"
        except tweepy.errors.NotFound:
            print(f"   â­ï¸ ãƒ„ã‚¤ãƒ¼ãƒˆå‰Šé™¤æ¸ˆã¿: {tweet_id}")
            replied_ids.add(tweet_id)
            result["status"] = "deleted"
        except Exception as e:
            print(f"   âŒ è¿”ä¿¡å¤±æ•—: {e}")
            result["status"] = f"failed: {e}"

        results.append(result)

    return results


def main():
    jst = timezone(timedelta(hours=9))
    now = datetime.now(jst)
    hour = now.hour

    # æ™‚é–“å¸¯ã«å¿œã˜ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    if 6 <= hour < 10:
        time_label = "ğŸŒ… æœã®ãƒã‚¸ãƒ‡ãƒ–"
    elif 11 <= hour < 14:
        time_label = "â˜€ï¸ æ˜¼ã®ãƒã‚¸ãƒ‡ãƒ–"
    else:
        time_label = "ğŸŒ™ å¤œã®ãƒã‚¸ãƒ‡ãƒ–"

    print("=" * 50)
    print(f"ğŸ– PosiDev Free Search - {time_label}")
    print(f"   {now.strftime('%Y-%m-%d %H:%M JST')}")
    print("=" * 50)

    write_client = get_write_client()
    read_client = get_read_client()

    if not write_client:
        print("âŒ X API credentials not set")
        return

    replied_ids = load_replied_ids()
    all_targets = []
    all_results = []

    # æ–¹å¼1: Googleæ¤œç´¢
    print("\nğŸ“¡ æ–¹å¼1: Googleæ¤œç´¢")
    google_tweets = search_google_for_tweets()
    all_targets.extend(google_tweets)
    print(f"   Googleç™ºè¦‹: {len(google_tweets)}ä»¶")

    # æ–¹å¼2: ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ç›£è¦–
    print("\nğŸ“© æ–¹å¼2: ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ç›£è¦–")
    if read_client:
        mention_tweets = check_mentions(read_client, write_client)
        all_targets.extend(mention_tweets)
        print(f"   ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³: {len(mention_tweets)}ä»¶")
    else:
        print("   âš ï¸ Bearer Tokenæœªè¨­å®š")

    # é‡è¤‡é™¤å»
    seen = set()
    unique_targets = []
    for t in all_targets:
        if t["id"] not in seen:
            seen.add(t["id"])
            unique_targets.append(t)

    print(f"\nğŸ¯ ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {len(unique_targets)}ä»¶")

    # è¿”ä¿¡å®Ÿè¡Œ
    if unique_targets:
        all_results = reply_to_tweets(write_client, read_client, unique_targets, replied_ids)

    save_replied_ids(replied_ids)

    sent_count = sum(1 for r in all_results if r["status"] == "sent")
    print(f"\nğŸ“Š çµæœ: {len(all_results)}ä»¶å‡¦ç†, {sent_count}ä»¶ãƒã‚¸ãƒ‡ãƒ–è¿”ä¿¡å®Œäº†")

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    generate_report(all_results, sent_count, time_label, now)
    print("âœ… Complete!")


def generate_report(results, sent_count, time_label, now):
    lines = [
        f"## ğŸ– ãƒã‚¸ãƒ‡ãƒ–è‡ªå‹•è¿”ä¿¡ãƒ¬ãƒãƒ¼ãƒˆ - {time_label}",
        "",
        f"**å®Ÿè¡Œæ—¥æ™‚:** {now.strftime('%Y-%m-%d %H:%M JST')}",
        f"**å‡¦ç†æ•°:** {len(results)}ä»¶",
        f"**è‡ªå‹•è¿”ä¿¡:** {sent_count}ä»¶",
        "",
        "---",
        "",
    ]

    if not results:
        lines.append("*æ–°ã—ã„ãƒã‚¬ãƒ‡ãƒ–ç™ºè¨€ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å¹³å’Œï¼ğŸ–*")
    else:
        for i, r in enumerate(results, 1):
            emoji = "âœ…" if r["status"] == "sent" else "âŒ"
            source = {"google": "Googleæ¤œç´¢", "mention": "ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³"}.get(r.get("source"), "ä¸æ˜")
            lines.append(f"### #{i} {emoji} [{source}]")
            lines.append(f"**@{r['username']}**: {r['text'][:150]}")
            lines.append(f"")
            lines.append(f"**ãƒã‚¸ãƒ‡ãƒ–è¿”ä¿¡** ({r['member']['name']} {r['member']['role']}):")
            lines.append(f"> {r['response']}")
            lines.append(f"")
            lines.append("---")
            lines.append("")

    lines.append("*Powered by DEV PARADE ãƒã‚¸ãƒ‡ãƒ–Bot ğŸ–*")

    with open("search_report.md", "w") as f:
        f.write("\n".join(lines))


if __name__ == "__main__":
    main()
